{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FPd0ch0fNYS"
      },
      "source": [
        "Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fUG8h_Teaxv",
        "outputId": "b94926d6-fe15-4ca2-9da1-3159c13bfd85"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "pip install tensorflow-addons\n",
        "pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Q0-Rswewj-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Disable progress bar for TensorFlow datasets\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Enable automatic tuning for performance optimization\n",
        "autotune = tf.data.AUTOTUNE\n",
        "\n",
        "# Set global mixed-precision policy for better performance\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQJoC9vdfn2l"
      },
      "source": [
        "# Preparing the dataset\n",
        "\n",
        "**Dataset Description:**  CGTrainTest is a mini version of Imagenet, ideal for training CycleGAN models for captivating image colorization.\n",
        "\n",
        "**Link to Dataset:** [Kaggle CGTrainTest](https://www.kaggle.com/datasets/eswardivi/cgtraintest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KP8Ne6nh4lT"
      },
      "source": [
        "Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3OR2L1ch655",
        "outputId": "de8fc2a2-4823-44b5-b22c-e1f4ea9b16f9"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Create the kaggle.json file with your credentials\n",
        "echo '{\"username\":\"eswardivi\",\"key\":\"fa0af4628f04f3801f29e7749a2acc51\"}' > kaggle.json\n",
        "\n",
        "# Move the kaggle.json file to the ~/.kaggle/ directory\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "# Set permissions for the kaggle.json file\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "kaggle datasets download -d eswardivi/cgtraintest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GriTwdfHiMwy"
      },
      "source": [
        "Extracting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4Jq5sS5iMR_",
        "outputId": "25a2e533-4d68-40e1-aa77-e38fb2d8a839"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "unzip ./cgtraintest.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG9scSaNih3b",
        "outputId": "75ee8e84-6231-4d78-da92-895ec1d5b1e6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_files_to_new_folder(source_folder, destination_folder, percentage=0.2):\n",
        "    # Create the destination folder if it doesn't exist\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "    # Get the list of files in the source folder\n",
        "    files = os.listdir(source_folder)\n",
        "\n",
        "    # Calculate the number of files to move (percentage of the total)\n",
        "    num_files_to_move = int(len(files) * percentage)\n",
        "\n",
        "    # Get the first percentage of files\n",
        "    files_to_move = files[:num_files_to_move]\n",
        "\n",
        "    # Move the selected files to the destination folder\n",
        "    for file_name in files_to_move:\n",
        "        source_path = os.path.join(source_folder, file_name)\n",
        "        destination_path = os.path.join(destination_folder, file_name)\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "    print(\"Files moved successfully.\")\n",
        "\n",
        "# Move 20% of files from the source folder to the destination folder\n",
        "move_files_to_new_folder('./trainA', './trainA_New')\n",
        "move_files_to_new_folder('./trainB', './trainB_New')\n",
        "move_files_to_new_folder('./testA', './testA_New')\n",
        "move_files_to_new_folder('./testB', './testB_New')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlA36QSj9FU",
        "outputId": "0134b0ae-9af0-4ee0-b5c0-3fdf9dbd6682"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "rm -rf trainA trainB testB testA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQxifzXQjmcm",
        "outputId": "3ba8befd-05b7-4d0b-e708-f3b75922327b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def convert_images_in_folder(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, os.path.splitext(filename)[0] + \".png\")\n",
        "\n",
        "        # Open the image and convert to PNG if the format is supported\n",
        "        img = Image.open(input_path)\n",
        "        img.save(output_path, \"PNG\")\n",
        "        print(f\"Converted {filename} to PNG.\")\n",
        "\n",
        "# Convert images in the specified folders\n",
        "convert_images_in_folder('./trainA_New', './trainA')\n",
        "convert_images_in_folder('./trainB_New', './trainB')\n",
        "convert_images_in_folder('./testA_New', './testA')\n",
        "convert_images_in_folder('./testB_New', './testB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SaFG-e-jBSA"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbgSL2bxmHjT",
        "outputId": "43bd757d-e6b1-45f6-d44c-e59882bf7617"
      },
      "outputs": [],
      "source": [
        "train_A=tf.keras.utils.image_dataset_from_directory('./trainA',label_mode=None,shuffle=False,class_names=None,batch_size=None,image_size=(224, 224))\n",
        "train_B=tf.keras.utils.image_dataset_from_directory('./trainB',label_mode=None,shuffle=False,class_names=None,batch_size=None,image_size=(224, 224))\n",
        "\n",
        "test_A=tf.keras.utils.image_dataset_from_directory('./testA',label_mode=None,shuffle=False,class_names=None,batch_size=None,image_size=(224, 224))\n",
        "test_B=tf.keras.utils.image_dataset_from_directory('./testB',label_mode=None,shuffle=False,class_names=None,batch_size=None,image_size=(224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgBoYwfkmP3F"
      },
      "outputs": [],
      "source": [
        "# Define the standard image size.\n",
        "orig_img_size = (224, 224)\n",
        "# Size of the random crops to be used during training.\n",
        "input_img_size = (224, 224, 3)\n",
        "# Weights initializer for the layers.\n",
        "kernel_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "# Gamma initializer for instance normalization.\n",
        "gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "buffer_size = 256\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float16)\n",
        "    # Map values in the range [-1, 1]\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "def preprocess_train_image(img):\n",
        "    # Random flip\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    # Resize to the original size first\n",
        "    img = tf.image.resize(img, [*orig_img_size])\n",
        "    # Normalize the pixel values in the range [-1, 1]\n",
        "    img = normalize_img(img)\n",
        "    return img\n",
        "\n",
        "def preprocess_test_image(img):\n",
        "    # Only resizing and normalization for the test images.\n",
        "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
        "    img = normalize_img(img)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmexWryomXXY"
      },
      "source": [
        "Creating Dataset Objects for Better Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCDpTo2pmbsX"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing operations to the training data for Domain A\n",
        "train_A = (\n",
        "    train_A.map(preprocess_train_image, num_parallel_calls=autotune)   # Preprocess each image in parallel\n",
        "    .cache()  # Cache the preprocessed data for faster retrieval during training\n",
        "    .shuffle(buffer_size)  # Shuffle the data to introduce randomness during training\n",
        "    .batch(batch_size)  # Group images into batches for training\n",
        ")\n",
        "train_B = (\n",
        "    train_B.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing operations to the test data\n",
        "test_A = (\n",
        "    test_A.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "test_B = (\n",
        "    test_B.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAXHBSY2mwRu"
      },
      "source": [
        "# Let us See Some Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dOWpG2QMmyZ7",
        "outputId": "693e96b9-00fd-4c43-e7d5-e25137ba96cb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def denormalize_img(img):\n",
        "    return ((img * 127.5) + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "def visualize_image_pairs(pair_list, title_list):\n",
        "    num_pairs = len(pair_list)\n",
        "\n",
        "    fig, axes = plt.subplots(num_pairs, 2, figsize=(10, 5 * num_pairs))\n",
        "    for i, (image_A, image_B) in enumerate(pair_list):\n",
        "        ax1, ax2 = axes[i]\n",
        "\n",
        "        if len(image_A.shape) == 4:  # Check if it's a batched image\n",
        "            image_A = image_A[0]  # Choose the first image from the batch\n",
        "            image_B = image_B[0]\n",
        "\n",
        "        ax1.imshow(denormalize_img(image_A))\n",
        "        ax1.set_title(title_list[0], fontsize=14)\n",
        "        ax1.axis(\"off\")\n",
        "\n",
        "        ax2.imshow(denormalize_img(image_B))\n",
        "        ax2.set_title(title_list[1], fontsize=14)\n",
        "        ax2.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create a list of image pairs from train_A and train_B\n",
        "image_pairs = list(zip(train_A.take(4), train_B.take(4)))\n",
        "\n",
        "# Titles for the image pairs\n",
        "titles = [\"GrayScale\", \"Color\"]\n",
        "\n",
        "# Visualize the image pairs\n",
        "visualize_image_pairs(image_pairs, titles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEey5KYCnbeb"
      },
      "source": [
        "# Making CycleGan Generators and Discriminators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRLzfptvniRb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "class ReflectionPadding2D(layers.Layer):\n",
        "    \"\"\"Implements Reflection Padding as a layer.\n",
        "\n",
        "    Args:\n",
        "        padding(tuple): Amount of padding for the spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        A padded tensor with the same type as the input tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        padding_width, padding_height = self.padding\n",
        "        padding_tensor = [\n",
        "            [0, 0],\n",
        "            [padding_height, padding_height],\n",
        "            [padding_width, padding_width],\n",
        "            [0, 0],\n",
        "        ]\n",
        "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
        "\n",
        "\n",
        "def residual_block(x, activation, kernel_initializer=kernel_init,\n",
        "                   kernel_size=(3, 3), strides=(1, 1), padding=\"valid\",\n",
        "                   gamma_initializer=gamma_init, use_bias=False):\n",
        "    dim = x.shape[-1]\n",
        "    input_tensor = x\n",
        "\n",
        "    x = ReflectionPadding2D()(input_tensor)\n",
        "    x = layers.Conv2D(dim, kernel_size, strides=strides,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      padding=padding, use_bias=use_bias)(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = activation(x)\n",
        "\n",
        "    x = ReflectionPadding2D()(x)\n",
        "    x = layers.Conv2D(dim, kernel_size, strides=strides,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      padding=padding, use_bias=use_bias)(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.add([input_tensor, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def downsample(x, filters, activation, kernel_initializer=kernel_init,\n",
        "               kernel_size=(3, 3), strides=(2, 2), padding=\"same\",\n",
        "               gamma_initializer=gamma_init, use_bias=False):\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=strides,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      padding=padding, use_bias=use_bias)(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def upsample(x, filters, activation, kernel_size=(3, 3), strides=(2, 2),\n",
        "             padding=\"same\", kernel_initializer=kernel_init,\n",
        "             gamma_initializer=gamma_init, use_bias=False):\n",
        "    x = layers.Conv2DTranspose(filters, kernel_size, strides=strides,\n",
        "                               padding=padding,\n",
        "                               kernel_initializer=kernel_initializer,\n",
        "                               use_bias=use_bias)(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxQ2Rc2Fnxql"
      },
      "source": [
        "# Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoN1L_TSnzdo"
      },
      "outputs": [],
      "source": [
        "def get_resnet_generator(\n",
        "    filters=64,\n",
        "    num_downsampling_blocks=3,\n",
        "    num_residual_blocks=5,\n",
        "    num_upsample_blocks=3,\n",
        "    gamma_initializer=gamma_init,\n",
        "    name=None,\n",
        "):\n",
        "    # Input layer for the generator model\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "\n",
        "    # Initial convolution block\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
        "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Downsampling blocks\n",
        "    for _ in range(num_downsampling_blocks):\n",
        "        filters *= 2\n",
        "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(num_residual_blocks):\n",
        "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Upsampling blocks\n",
        "    for _ in range(num_upsample_blocks):\n",
        "        filters //= 2\n",
        "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Final block\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
        "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)  # Output with 3 channels for RGB images\n",
        "    x = layers.Activation(\"tanh\")(x)  # Output activation to map values to [-1, 1]\n",
        "\n",
        "    # Create the generator model\n",
        "    model = tf.keras.models.Model(img_input, x, name=name)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6uCCx2En6Wc"
      },
      "source": [
        "# Discriminators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hxl8KFroeUM"
      },
      "outputs": [],
      "source": [
        "def get_discriminator(\n",
        "    filters=64, kernel_initializer=kernel_init, num_downsampling=5, name=None\n",
        "):\n",
        "    # Input layer for the discriminator model\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "\n",
        "    # First convolutional block\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        (4, 4),\n",
        "        strides=(2, 2),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_initializer,\n",
        "    )(img_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Downsampling blocks\n",
        "    num_filters = filters\n",
        "    for num_downsample_block in range(num_downsampling):\n",
        "        num_filters *= 2\n",
        "        if num_downsample_block < 2:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(2, 2),\n",
        "            )\n",
        "        else:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(1, 1),\n",
        "            )\n",
        "\n",
        "    # Final convolutional block\n",
        "    x = layers.Conv2D(\n",
        "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
        "    )(x)\n",
        "\n",
        "    # Create the discriminator model\n",
        "    model = tf.keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ET0AF8SmojLl",
        "outputId": "a705bebe-0af1-41f7-c4ac-a9c808cb4fc3"
      },
      "outputs": [],
      "source": [
        "# Get the generators\n",
        "gen_G = get_resnet_generator(name=\"generator_G\")\n",
        "gen_F = get_resnet_generator(name=\"generator_F\")\n",
        "tf.keras.utils.plot_model(gen_G, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ApqCanKGo_d2",
        "outputId": "39368380-6ba9-4369-9328-8138bfb5bc58"
      },
      "outputs": [],
      "source": [
        "# Get the discriminators\n",
        "disc_X = get_discriminator(name=\"discriminator_X\")\n",
        "disc_Y = get_discriminator(name=\"discriminator_Y\")\n",
        "tf.keras.utils.plot_model(disc_X, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aHCKU-opWMc"
      },
      "source": [
        "# Cycle GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJhPEw9ApmvU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator_G,\n",
        "        generator_F,\n",
        "        discriminator_X,\n",
        "        discriminator_Y,\n",
        "        lambda_cycle=10.0,\n",
        "        lambda_identity=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.gen_G = generator_G\n",
        "        self.gen_F = generator_F\n",
        "        self.disc_X = discriminator_X\n",
        "        self.disc_Y = discriminator_Y\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_identity = lambda_identity\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return (\n",
        "            self.disc_X(inputs),\n",
        "            self.disc_Y(inputs),\n",
        "            self.gen_G(inputs),\n",
        "            self.gen_F(inputs),\n",
        "        )\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        gen_G_optimizer,\n",
        "        gen_F_optimizer,\n",
        "        disc_X_optimizer,\n",
        "        disc_Y_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "    ):\n",
        "        super().compile()\n",
        "        self.gen_G_optimizer = gen_G_optimizer\n",
        "        self.gen_F_optimizer = gen_F_optimizer\n",
        "        self.disc_X_optimizer = disc_X_optimizer\n",
        "        self.disc_Y_optimizer = disc_Y_optimizer\n",
        "        self.generator_loss_fn = gen_loss_fn\n",
        "        self.discriminator_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
        "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        # x is Grayscale and y is Color\n",
        "        real_x, real_y = batch_data\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            fake_y = self.gen_G(real_x, training=True)\n",
        "            fake_x = self.gen_F(real_y, training=True)\n",
        "\n",
        "            cycled_x = self.gen_F(fake_y, training=True)\n",
        "            cycled_y = self.gen_G(fake_x, training=True)\n",
        "\n",
        "            # Identity mapping\n",
        "            same_x = self.gen_F(real_x, training=True)\n",
        "            same_y = self.gen_G(real_y, training=True)\n",
        "\n",
        "            # Discriminator output\n",
        "            disc_real_x = self.disc_X(real_x, training=True)\n",
        "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
        "\n",
        "            disc_real_y = self.disc_Y(real_y, training=True)\n",
        "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
        "\n",
        "            # Generator adversarial loss\n",
        "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
        "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
        "\n",
        "            # Generator cycle loss\n",
        "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
        "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
        "\n",
        "            # Generator identity loss\n",
        "            id_loss_G = (\n",
        "                self.identity_loss_fn(real_y, same_y)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "            id_loss_F = (\n",
        "                self.identity_loss_fn(real_x, same_x)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "\n",
        "            # Total generator loss\n",
        "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
        "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
        "\n",
        "            # Discriminator loss\n",
        "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
        "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
        "\n",
        "        # Get the gradients for the generators\n",
        "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
        "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
        "\n",
        "        # Get the gradients for the discriminators\n",
        "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
        "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
        "\n",
        "        # Update the weights of the generators\n",
        "        self.gen_G_optimizer.apply_gradients(\n",
        "            zip(grads_G, self.gen_G.trainable_variables)\n",
        "        )\n",
        "        self.gen_F_optimizer.apply_gradients(\n",
        "            zip(grads_F, self.gen_F.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # Update the weights of the discriminators\n",
        "        self.disc_X_optimizer.apply_gradients(\n",
        "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
        "        )\n",
        "        self.disc_Y_optimizer.apply_gradients(\n",
        "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"G_loss\": total_loss_G,\n",
        "            \"F_loss\": total_loss_F,\n",
        "            \"D_X_loss\": disc_X_loss,\n",
        "            \"D_Y_loss\": disc_Y_loss,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bFTfrwvqI_m"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqpb6TO3p5Rx"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
        "\n",
        "    def __init__(self, num_img=4, output_dir=\"generated_images\"):\n",
        "        self.num_img = num_img\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n",
        "        for i, img in enumerate(train_A.take(self.num_img)):   # For Testing\n",
        "            prediction = self.model.gen_G(img)[0].numpy()\n",
        "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "            ax[i, 0].imshow(img)\n",
        "            ax[i, 1].imshow(prediction)\n",
        "            ax[i, 0].set_title(\"Input image\")\n",
        "            ax[i, 1].set_title(\"Translated image\")\n",
        "            ax[i, 0].axis(\"off\")\n",
        "            ax[i, 1].axis(\"off\")\n",
        "\n",
        "            prediction = keras.preprocessing.image.array_to_img(prediction)\n",
        "            image_path = os.path.join(self.output_dir, f\"generated_img_{i}_{epoch + 1}.png\")\n",
        "            prediction.save(image_path)\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p33smXIXqLV-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UGUb-PWqN1r"
      },
      "outputs": [],
      "source": [
        "# Loss function for evaluating adversarial loss\n",
        "adv_loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "# Define the loss function for the generators\n",
        "def generator_loss_fn(fake):\n",
        "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
        "    return fake_loss\n",
        "\n",
        "\n",
        "# Define the loss function for the discriminators\n",
        "def discriminator_loss_fn(real, fake):\n",
        "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
        "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
        "    return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "\n",
        "# Create cycle gan model\n",
        "cycle_gan_model = CycleGan(\n",
        "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJestw6qqWUG"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "cycle_gan_model.compile(\n",
        "    gen_G_optimizer=keras.optimizers.legacy.Adam(learning_rate=3e-4, beta_1=0.5),\n",
        "    gen_F_optimizer=keras.optimizers.legacy.Adam(learning_rate=3e-4, beta_1=0.5),\n",
        "    disc_X_optimizer=keras.optimizers.legacy.Adam(learning_rate=3e-4, beta_1=0.5),\n",
        "    disc_Y_optimizer=keras.optimizers.legacy.Adam(learning_rate=3e-4, beta_1=0.5),\n",
        "    gen_loss_fn=generator_loss_fn,\n",
        "    disc_loss_fn=discriminator_loss_fn,\n",
        ")\n",
        "# Callbacks\n",
        "plotter = GANMonitor()\n",
        "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHfKBjWIq_VO"
      },
      "source": [
        "# Final call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atwwhhX-q_EA",
        "outputId": "dd7c20c5-bbf9-4b30-80c6-1aac165ddc96"
      },
      "outputs": [],
      "source": [
        "cycle_gan_model.fit(\n",
        "    tf.data.Dataset.zip((train_A, train_B)),\n",
        "    epochs=25,\n",
        "    callbacks=[plotter, model_checkpoint_callback],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk0Ki1W9rD4G"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pI4GxHwrFgE"
      },
      "outputs": [],
      "source": [
        "# Load the checkpoints\n",
        "weight_file = \"./model_checkpoints/cyclegan_checkpoints.002\"\n",
        "cycle_gan_model.load_weights(weight_file).expect_partial()\n",
        "print(\"Weights loaded successfully\")\n",
        "\n",
        "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
        "for i, img in enumerate(test_A.take(4)):\n",
        "    prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "    ax[i, 0].imshow(img)\n",
        "    ax[i, 1].imshow(prediction)\n",
        "    ax[i, 0].set_title(\"Input image\")\n",
        "    ax[i, 0].set_title(\"Input image\")\n",
        "    ax[i, 1].set_title(\"Translated image\")\n",
        "    ax[i, 0].axis(\"off\")\n",
        "    ax[i, 1].axis(\"off\")\n",
        "\n",
        "    prediction = keras.utils.array_to_img(prediction)\n",
        "    prediction.save(\"predicted_img_{i}.png\".format(i=i))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
